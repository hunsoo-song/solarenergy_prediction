{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import pathlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "\n",
    "os.chdir(\"C:\\\\Users\\\\Hunsoo\\\\Desktop\\\\regression_practice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 기상데이터 인덱스에 맞게 영상 불러오기 ## 여기로 부터 저장된거에  Multi temporal data로 만들어주고 이름 \"raw_dataset_with_coms_pix_multitemporal\"로 저장했음\n",
    "\n",
    "##### Multitemporal로 저장해준거 불러오기 이름은 아래처럼 ##########\n",
    "# coms_data_ir1 coms_data_ir2  coms_data_wv coms_data_swir coms_data_vis   (현시각, 기본)\n",
    "# coms_data_ir1_1hr coms_data_ir2_1hr coms_data_wv_1hr coms_data_swir_1hr coms_data_vis_1hr  (1시간전)\n",
    "# .....\n",
    "# coms_data_ir1_4hr coms_data_ir2_4hr coms_data_wv_4hr coms_data_swir_4hr coms_data_vis_4hr  (4시간전)\n",
    "\n",
    "raw_dataset_with_coms_pix_multitemporal=pd.read_excel('raw_dataset_with_coms_pix_multitemporal.xlsx')\n",
    "raw_dataset_with_coms_pix_multitemporal.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 걍 raw_dataset으로 이름 다시 바꿔주기 ###\n",
    "raw_dataset= copy.deepcopy(raw_dataset_with_coms_pix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## clock 01, 02, 03 --> 1, 2, 3\n",
    "raw_dataset.clock=pd.to_numeric(raw_dataset.clock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 시간 솎아주기\n",
    "\n",
    "raw_dataset.drop(raw_dataset[raw_dataset.clock<9].index,inplace=True)\n",
    "raw_dataset.drop(raw_dataset[raw_dataset.clock>19].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 걍 이름 한번 바꿈 ;;;\n",
    "dataset=raw_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 걍 이름 한번 바꿈 ;;;\n",
    "\n",
    "dataset.drop(dataset[dataset.coms_data_ir1=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_ir2=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_wv=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_swir=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_vis=='skip'].index,inplace=True)\n",
    "\n",
    "dataset.drop(dataset[dataset.coms_data_ir1_1hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_ir2_1hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_wv_1hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_swir_1hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_vis_1hr=='skip'].index,inplace=True)\n",
    "\n",
    "dataset.drop(dataset[dataset.coms_data_ir1_2hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_ir2_2hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_wv_2hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_swir_2hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_vis_2hr=='skip'].index,inplace=True)\n",
    "\n",
    "dataset.drop(dataset[dataset.coms_data_ir1_3hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_ir2_3hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_wv_3hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_swir_3hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_vis_3hr=='skip'].index,inplace=True)\n",
    "\n",
    "dataset.drop(dataset[dataset.coms_data_ir1_4hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_ir2_4hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_wv_4hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_swir_4hr=='skip'].index,inplace=True)\n",
    "dataset.drop(dataset[dataset.coms_data_vis_4hr=='skip'].index,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################################################################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"fields_to_drop = ['point','year','month','day','day365','clock','GMT','temp','windspeed','winddirection','humidity',                  'vaporpressure','dewpointtemperature','locarpressure','seasurfacepressure','sunshine',                  'irradiation','snow','cloud','cloud2','lowcloud','sijung','lst','temp5cm','temp10cm',                  'temp20','temp30cm','pv_5h','pv_4h','pv_3h','pv_2h',pv_1h','pv0h','pv1h','pv2h','pv3h','pv4h','pv5h',                  'coms_data_ir1','coms_data_ir2','coms_data_wv','coms_data_swir','coms_data_vis',                  'coms_data_ir1_1hr','coms_data_ir2_1hr','coms_data_wv_1hr','coms_data_swir_1hr','coms_data_vis_1hr',                  'coms_data_ir1_2hr','coms_data_ir2_2hr','coms_data_wv_2hr','coms_data_swir_2hr','coms_data_vis_2hr',                  'coms_data_ir1_3hr','coms_data_ir2_3hr','coms_data_wv_3hr','coms_data_swir_3hr','coms_data_vis_3hr',                  'coms_data_ir1_4hr','coms_data_ir2_4hr','coms_data_wv_4hr','coms_data_swir_4hr','coms_data_vis_4hr']\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### feature 모음 (기록용) ###\n",
    "\"\"\"fields_to_drop = ['point','year','month','day','day365','clock','GMT','temp','windspeed','winddirection','humidity',\\\n",
    "                  'vaporpressure','dewpointtemperature','locarpressure','seasurfacepressure','sunshine',\\\n",
    "                  'irradiation','snow','cloud','cloud2','lowcloud','sijung','lst','temp5cm','temp10cm',\\\n",
    "                  'temp20','temp30cm','pv_5h','pv_4h','pv_3h','pv_2h',pv_1h','pv0h','pv1h','pv2h','pv3h','pv4h','pv5h',\\\n",
    "                  'coms_data_ir1','coms_data_ir2','coms_data_wv','coms_data_swir','coms_data_vis',\\\n",
    "                  'coms_data_ir1_1hr','coms_data_ir2_1hr','coms_data_wv_1hr','coms_data_swir_1hr','coms_data_vis_1hr',\\\n",
    "                  'coms_data_ir1_2hr','coms_data_ir2_2hr','coms_data_wv_2hr','coms_data_swir_2hr','coms_data_vis_2hr',\\\n",
    "                  'coms_data_ir1_3hr','coms_data_ir2_3hr','coms_data_wv_3hr','coms_data_swir_3hr','coms_data_vis_3hr',\\\n",
    "                  'coms_data_ir1_4hr','coms_data_ir2_4hr','coms_data_wv_4hr','coms_data_swir_4hr','coms_data_vis_4hr']\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 실험에 넣고픈거 ###\n",
    "\n",
    "fields_to_drop = ['point','year','month','day','day365','clock','GMT','temp','windspeed','winddirection','humidity',\\\n",
    "                  'vaporpressure','dewpointtemperature','locarpressure','seasurfacepressure','sunshine',\\\n",
    "                  'irradiation','snow','cloud','cloud2','lowcloud','sijung','lst','temp5cm','temp10cm',\\\n",
    "                  'temp20','temp30cm','pv_5h','pv_4h','pv_3h','pv_2h','pv1h','pv3h','pv4h','pv5h',\\\n",
    "                  'coms_data_ir1_1hr','coms_data_ir2_1hr','coms_data_wv_1hr','coms_data_swir_1hr','coms_data_vis_1hr',\\\n",
    "                  'coms_data_ir1_3hr','coms_data_ir2_3hr','coms_data_wv_3hr','coms_data_swir_3hr','coms_data_vis_3hr']\n",
    "                  \n",
    "data = dataset.drop(fields_to_drop, axis=1)\n",
    "data.head()\n",
    "\n",
    "dataset = data.copy()\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.astype(float)\n",
    "dataset=dataset.dropna()\n",
    "dataset.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### 밴드별 히스토그램 봐보기 ########"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_coms_data_ir1=np.array(dataset['coms_data_ir1'])\n",
    "plt.hist(np_coms_data_ir1, bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_coms_data_ir2=np.array(dataset['coms_data_ir2'])\n",
    "plt.hist(np_coms_data_ir2, bins='auto')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_coms_data_wv=np.array(dataset['coms_data_wv'])\n",
    "plt.hist(np_coms_data_wv, bins='auto')\n",
    "#plt.hist(np_coms_data_wv, bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_coms_data_swir=np.array(dataset['coms_data_swir'])\n",
    "plt.hist(np_coms_data_swir, bins='auto')\n",
    "#plt.hist(np_coms_data_swir, bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np_coms_data_vis=np.array(dataset['coms_data_vis'])\n",
    "plt.hist(np_coms_data_vis, bins='auto')\n",
    "#plt.hist(np_coms_data_vis, bins=500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### 3sigma 이상 값 빼주는거 (안해도될듯?) ###\n",
    "\n",
    "\"\"\"from scipy import stats\n",
    "dataset_preprocessed=dataset[(np.abs(stats.zscore(dataset['coms_data_ir1'])) < 3)]\n",
    "\n",
    "np.shape(dataset_preprocessed)\n",
    "\n",
    "dataset_preprocessed=dataset_preprocessed[(np.abs(stats.zscore(dataset_preprocessed['coms_data_ir2'])) < 3)]\n",
    "\n",
    "np.shape(dataset_preprocessed)\n",
    "\n",
    "dataset_preprocessed=dataset_preprocessed[(np.abs(stats.zscore(dataset_preprocessed['coms_data_wv'])) < 3)]\n",
    "\n",
    "np.shape(dataset_preprocessed)\n",
    "\n",
    "dataset_preprocessed=dataset_preprocessed[(np.abs(stats.zscore(dataset_preprocessed['coms_data_swir'])) < 3)]\n",
    "\n",
    "np.shape(dataset_preprocessed)\n",
    "\n",
    "dataset_preprocessed=dataset_preprocessed[(np.abs(stats.zscore(dataset_preprocessed['coms_data_vis'])) < 3)]\n",
    "\n",
    "np.shape(dataset_preprocessed)\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 그냥 이름 바꿈;;; 전처리했을수도잇으니깐\n",
    "dataset_preprocessed=dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## MIN MAX Normalization 해주기\n",
    "\n",
    "\n",
    "quant_features = ['pv_1h','pv0h','pv2h',\\\n",
    "                  'coms_data_ir1','coms_data_ir2','coms_data_wv','coms_data_swir','coms_data_vis',\\\n",
    "                  'coms_data_ir1_2hr','coms_data_ir2_2hr','coms_data_wv_2hr','coms_data_swir_2hr','coms_data_vis_2hr',\\\n",
    "                  'coms_data_ir1_4hr','coms_data_ir2_4hr','coms_data_wv_4hr','coms_data_swir_4hr','coms_data_vis_4hr']\n",
    "\n",
    "scaled_features = {}\n",
    "\n",
    "#for each in quant_features:\n",
    "#    mean, std = data[each].mean(), data[each].std()\n",
    "#    scaled_features[each] = [mean, std]\n",
    "#    data.loc[:, each] = (data[each] - mean)/std\n",
    "\n",
    "for each in quant_features:\n",
    "    maxx, minn = dataset_preprocessed[each].max(), dataset_preprocessed[each].min()\n",
    "    scaled_features[each] = [maxx, minn]\n",
    "    dataset_preprocessed.loc[:, each] = (dataset_preprocessed[each] - minn)/(maxx-minn)\n",
    "    \n",
    "#for each in dataset_preprocessed:\n",
    "#    maxx, minn = dataset_preprocessed[each].max(), dataset_preprocessed[each].min()\n",
    "#    scaled_features[each] = [maxx, minn]\n",
    "#    dataset_preprocessed.loc[:, each] = (dataset_preprocessed[each] - minn)/(maxx-minn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Min MAx값 보여주기\n",
    "scaled_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset_preprocessed.sample(frac=0.90,random_state=9)\n",
    "test_dataset = dataset_preprocessed.drop(train_dataset.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 뭘테스트? pv2h\n",
    "train_labels = train_dataset.pop('pv2h')\n",
    "test_labels = test_dataset.pop('pv2h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(64, activation=tf.nn.relu, input_shape=[dataset_preprocessed.shape[1]-1]),\n",
    "    layers.Dense(64, activation=tf.nn.relu),\n",
    "    layers.Dense(1)\n",
    "  ])\n",
    "\n",
    "  optimizer = tf.keras.optimizers.RMSprop(0.001)\n",
    "\n",
    "  model.compile(loss='mean_squared_error',\n",
    "                optimizer=optimizer,\n",
    "                metrics=['mean_absolute_error', 'mean_squared_error'])\n",
    "  return model\n",
    "\n",
    "model = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TEST ## 에포크가 끝날 때마다 점(.)을 출력해 훈련 진행 과정을 표시합니다\n",
    "class PrintDot(keras.callbacks.Callback):\n",
    "  def on_epoch_end(self, epoch, logs):\n",
    "    if epoch % 1 == 0: print('')\n",
    "    print('.', end='')\n",
    "\n",
    "EPOCHS = 100\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "\n",
    "history = model.fit(train_dataset, train_labels, epochs=EPOCHS,\n",
    "                    validation_split = 0.2, verbose=0, callbacks=[early_stop, PrintDot()])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist = pd.DataFrame(history.history)\n",
    "hist['epoch'] = history.epoch\n",
    "hist.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history):\n",
    "  hist = pd.DataFrame(history.history)\n",
    "  hist['epoch'] = history.epoch\n",
    "  \n",
    "  plt.figure(figsize=(8,12))\n",
    "  \n",
    "  plt.subplot(2,1,1)\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Abs Error [pv+2h]')\n",
    "  plt.plot(hist['epoch'], hist['mean_absolute_error'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mean_absolute_error'],\n",
    "           label = 'Val Error')\n",
    "  plt.ylim([0,0.5])\n",
    "  plt.legend()\n",
    "  \n",
    "  plt.subplot(2,1,2)\n",
    "  plt.xlabel('Epoch')\n",
    "  plt.ylabel('Mean Square Error [$pv+2h^2$]')\n",
    "  plt.plot(hist['epoch'], hist['mean_squared_error'],\n",
    "           label='Train Error')\n",
    "  plt.plot(hist['epoch'], hist['val_mean_squared_error'],\n",
    "           label = 'Val Error')\n",
    "  plt.ylim([0,0.05])\n",
    "  plt.legend()\n",
    "  plt.show()\n",
    "\n",
    "plot_history(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, mae, mse = model.evaluate(test_dataset, test_labels, verbose=0)\n",
    "\n",
    "print(\"테스트 세트의 평균 절대 오차: {:5.2f} pv+2h\".format(mae))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = model.predict(test_dataset).flatten()\n",
    "\n",
    "plt.scatter(test_labels, test_predictions)\n",
    "plt.xlabel('True Values [pv+2h]')\n",
    "plt.ylabel('Predictions [pv+2h]')\n",
    "plt.axis('equal')\n",
    "plt.axis('square')\n",
    "plt.xlim([0,plt.xlim()[1]])\n",
    "plt.ylim([0,plt.ylim()[1]])\n",
    "_ = plt.plot([-100, 5000], [-100, 5000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = test_predictions - test_labels\n",
    "plt.hist(error, bins = 25)\n",
    "plt.xlabel(\"Prediction Error [pv+2h]\")\n",
    "_ = plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array_test_labels=pd.Series(test_labels).values\n",
    "array_test_predictions=pd.Series(test_predictions).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## RMSE\n",
    "rss=((array_test_labels-array_test_predictions)**2).sum()\n",
    "mse=np.mean((array_test_labels-array_test_predictions)**2)\n",
    "print(\"Final rmse value is =\",np.sqrt(np.mean((array_test_labels-array_test_predictions)**2)))\n",
    "rmse=np.sqrt(np.mean((array_test_labels-array_test_predictions)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CORR COEF\n",
    "corr=np.corrcoef(array_test_labels,array_test_predictions)\n",
    "corr[1,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### MAPE\n",
    "\n",
    "def mape_vectorized_v2(a, b): \n",
    "    mask = a != 0\n",
    "    return (np.fabs(a - b)/a)[mask].mean()\n",
    "\n",
    "mape_vectorized_v2(array_test_labels,array_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### nMAE\n",
    "\n",
    "def nmae(a, b, c): \n",
    "    mask = a != 0\n",
    "    return (np.fabs(a - b)/max(c))[mask].mean()\n",
    "\n",
    "nmae(array_test_labels,array_test_predictions,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### nMAE\n",
    "\n",
    "def nmae_wo_mask(a, b, c): \n",
    "    return (np.fabs(a - b)/max(c)).mean()\n",
    "\n",
    "nmae_wo_mask(array_test_labels,array_test_predictions,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Results 모음   ### RMSE CORRCOEF MAPE NMAE NMAE 순!!\n",
    "\n",
    "results=[]\n",
    "results.append(rmse)\n",
    "results.append(corr[1,0])\n",
    "results.append(mape_vectorized_v2(array_test_labels,array_test_predictions))\n",
    "results.append(nmae(array_test_labels,array_test_predictions,test_labels))\n",
    "results.append(nmae_wo_mask(array_test_labels,array_test_predictions,test_labels))\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
